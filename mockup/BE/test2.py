import time
import whisper
import edge_tts
import asyncio
import os
import torch
import pygame
import json
import librosa
import noisereduce as nr
import numpy as np
import soundfile as sf
from scipy import signal
from transformers import AutoTokenizer

# VoiceCommandModelì€ ë°˜ë“œì‹œ ë³„ë„ íŒŒì¼ì—ì„œ importí•´ì•¼ í•¨
from voice_command_model import VoiceCommandModel

# ============================================
# ìŒì„± ì „ì²˜ë¦¬ í´ëž˜ìŠ¤
# ============================================
class AudioPreprocessor:
    """ìŒì„± ì „ì²˜ë¦¬ í´ëž˜ìŠ¤"""
    
    def __init__(self, target_sr=16000):
        self.target_sr = target_sr
    
    def load_audio(self, file_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ"""
        print(f"   ðŸ“‚ íŒŒì¼ ë¡œë”©: {os.path.basename(file_path)}")
        try:
            # soundfileë¡œ ë¨¼ì € ì‹œë„ (ë” ë¹ ë¦„)
            audio, sr = sf.read(file_path)
            if len(audio.shape) > 1:  # ìŠ¤í…Œë ˆì˜¤ â†’ ëª¨ë…¸
                audio = audio.mean(axis=1)
        except:
            # ì‹¤íŒ¨í•˜ë©´ librosaë¡œ (ë” ë§Žì€ í¬ë§· ì§€ì›)
            audio, sr = librosa.load(file_path, sr=None, mono=True)
        
        print(f"      ì›ë³¸ SR: {sr}Hz, ê¸¸ì´: {len(audio)/sr:.2f}ì´ˆ")
        return audio, sr
    
    def resample(self, audio, orig_sr):
        """ë¦¬ìƒ˜í”Œë§"""
        if orig_sr != self.target_sr:
            print(f"   ðŸ”„ ë¦¬ìƒ˜í”Œë§: {orig_sr}Hz â†’ {self.target_sr}Hz")
            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=self.target_sr)
        return audio
    
    def remove_noise(self, audio, sr, stationary=True):
        """ìž¡ìŒ ì œê±°"""
        print(f"   ðŸ”‡ ìž¡ìŒ ì œê±° ì¤‘...")
        reduced_noise = nr.reduce_noise(
            y=audio, 
            sr=sr,
            stationary=stationary,
            prop_decrease=0.8
        )
        return reduced_noise
    
    def normalize_volume(self, audio, target_dBFS=-20.0):
        """ìŒëŸ‰ ì •ê·œí™”"""
        print(f"   ðŸ”Š ìŒëŸ‰ ì •ê·œí™” ì¤‘... (ëª©í‘œ: {target_dBFS}dBFS)")
        rms = np.sqrt(np.mean(audio**2))
        current_dBFS = 20 * np.log10(rms) if rms > 0 else -np.inf
        
        target_rms = 10 ** (target_dBFS / 20)
        gain = target_rms / (rms + 1e-10)
        normalized = audio * gain
        
        # í´ë¦¬í•‘ ë°©ì§€
        max_val = np.max(np.abs(normalized))
        if max_val > 1.0:
            normalized = normalized / max_val * 0.95
        
        print(f"      ì´ì „: {current_dBFS:.1f}dBFS â†’ ì´í›„: {target_dBFS:.1f}dBFS")
        return normalized
    
    def remove_silence(self, audio, sr, threshold_db=-40):
        """ë¬´ìŒ êµ¬ê°„ ì œê±°"""
        print(f"   âœ‚ï¸  ë¬´ìŒ êµ¬ê°„ ì œê±° ì¤‘...")
        intervals = librosa.effects.split(
            audio,
            top_db=-threshold_db,
            frame_length=2048,
            hop_length=512
        )
        
        trimmed = np.concatenate([audio[start:end] for start, end in intervals])
        removed_duration = (len(audio) - len(trimmed)) / sr
        print(f"      ì œê±°: {removed_duration:.2f}ì´ˆ, ìµœì¢…: {len(trimmed)/sr:.2f}ì´ˆ")
        return trimmed
    
    def apply_bandpass_filter(self, audio, sr, lowcut=80, highcut=7500):
        """ëŒ€ì—­í†µê³¼ í•„í„° (ìŒì„± ì£¼íŒŒìˆ˜ë§Œ í†µê³¼)"""
        print(f"   ðŸŽ›ï¸  ëŒ€ì—­í†µê³¼ í•„í„°: {lowcut}Hz ~ {highcut}Hz")
        nyquist = sr / 2
        low = lowcut / nyquist
        high = min(highcut / nyquist, 0.99)  # Nyquist ì£¼íŒŒìˆ˜ ë¯¸ë§Œìœ¼ë¡œ ì œí•œ
        
        # ì£¼íŒŒìˆ˜ ë²”ìœ„ ìœ íš¨ì„± ê²€ì‚¬
        if low >= high:
            print(f"      âš ï¸  í•„í„° ë²”ìœ„ ì˜¤ë¥˜, í•„í„°ë§ ìŠ¤í‚µ")
            return audio
        
        try:
            b, a = signal.butter(5, [low, high], btype='band')
            filtered = signal.filtfilt(b, a, audio)
            return filtered
        except ValueError as e:
            print(f"      âš ï¸  í•„í„°ë§ ì‹¤íŒ¨: {e}, ì›ë³¸ ë°˜í™˜")
            return audio
    
    def preprocess(self, file_path, output_path=None):
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("\nðŸŽ™ï¸  ìŒì„± ì „ì²˜ë¦¬ ì‹œìž‘")
        preprocess_start = time.time()
        
        # 1. ë¡œë“œ
        audio, sr = self.load_audio(file_path)
        
        # 2. ë¦¬ìƒ˜í”Œë§
        audio = self.resample(audio, sr)
        sr = self.target_sr
        
        # 3. ìž¡ìŒ ì œê±°
        audio = self.remove_noise(audio, sr)
        
        # 4. ëŒ€ì—­í†µê³¼ í•„í„°
        audio = self.apply_bandpass_filter(audio, sr)
        
        # 5. ë¬´ìŒ êµ¬ê°„ ì œê±°
        audio = self.remove_silence(audio, sr)
        
        # 6. ìŒëŸ‰ ì •ê·œí™” (ë§ˆì§€ë§‰ì—)
        audio = self.normalize_volume(audio)
        
        # 7. ì €ìž¥
        if output_path:
            sf.write(output_path, audio, sr)
            print(f"   ðŸ’¾ ì €ìž¥: {os.path.basename(output_path)}")
        
        preprocess_time = time.time() - preprocess_start
        print(f"âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ ({preprocess_time:.2f}ì´ˆ)\n")
        
        return audio, sr, preprocess_time


# ============================================
# ì„¤ì •
# ============================================
save_path = r"C:\Users\SSAFY\í¥ë¶€ìž\S13P31A103\mockup\BE\saved_voice_command_model"

# ============================================
# GPU ì„¤ì • í™•ì¸
# ============================================
print("=" * 60)
print("ðŸ–¥ï¸  GPU ì„¤ì • í™•ì¸")
print("=" * 60)
print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
else:
    print("âš ï¸  GPU ì—†ìŒ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")

# ============================================
# ëª¨ë¸ ë¡œë”©
# ============================================
print("\n" + "=" * 60)
print("ðŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
print("=" * 60)

with open(f"{save_path}/config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

ner_tag2id = config["ner_tag2id"]
ner_id2tag = {v: k for k, v in ner_tag2id.items()}
intent2id = config["intent2id"]
id2intent = {v: k for k, v in intent2id.items()}
model_name = config["model_name"]

print(f"âœ“ Config ë¡œë“œ ì™„ë£Œ")
print(f"  - NER Tags: {len(ner_tag2id)}ê°œ")
print(f"  - Intents: {len(intent2id)}ê°œ")
print(f"  - Base Model: {model_name}")

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(save_path)
print(f"âœ“ Tokenizer ë¡œë“œ ì™„ë£Œ")

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í›„ ê°€ì¤‘ì¹˜ ë¡œë“œ
model_ner_intent = VoiceCommandModel(
    model_name=model_name,
    num_ner_tags=len(ner_tag2id),
    num_intents=len(intent2id)
)
model_ner_intent.load_state_dict(
    torch.load(f"{save_path}/pytorch_model.bin", map_location="cpu")
)
model_ner_intent.eval()
print(f"âœ“ NER/Intent ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# Whisper ëª¨ë¸ ë¡œë”©
print(f"âœ“ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
model_whisper = whisper.load_model("medium")
print(f"âœ“ Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
preprocessor = AudioPreprocessor(target_sr=16000)
print(f"âœ“ ìŒì„± ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def generate_feedback(command_type):
    """í”¼ë“œë°± ë©”ì‹œì§€ ìƒì„±"""
    feedbacks = {
        "PLAY_MUSIC": "ë„¤~ ë…¸ëž˜ í‹€ì–´ë“œë¦´ê²Œìš”!",
        "START_EXERCISE": "ì¢‹ì•„ìš”! ì²´ì¡° ëª¨ë“œë¡œ ë°”ê¿€ê²Œìš”.",
        "PAUSE": "ìž ê¹ ë©ˆì¶œê²Œìš”.",
        "RESUME": "ê³„ì† ë“¤ë ¤ë“œë¦´ê²Œìš”~",
        "NEXT_SONG": "ë‹¤ìŒ ê³¡ìœ¼ë¡œ ë„˜ì–´ê°ˆê²Œìš”!",
        "STOP_ALL": "ì•Œê² ì–´ìš”. ì¢…ë£Œí• ê²Œìš”.",
        "START_LISTENING": "ìŒì•… ê°ìƒ ëª¨ë“œë¡œ ë°”ê¿€ê²Œìš”.",
        "SWITCH_TO_EXERCISE": "ìš´ë™ ëª¨ë“œë¡œ ì „í™˜í• ê²Œìš”.",
        "SWITCH_TO_LISTENING": "ìŒì•… ê°ìƒ ëª¨ë“œë¡œ ì „í™˜í• ê²Œìš”.",
        "EMERGENCY": "ê´œì°®ìœ¼ì„¸ìš”? ëŒ€ë‹µí•´ì£¼ì„¸ìš”! ì§€ê¸ˆ ë„ì›€ì„ ìš”ì²­í• ê²Œìš”!",
        "UNKNOWN": "ìž˜ ëª» ë“¤ì—ˆì–´ìš”. ë‹¤ì‹œ í•œë²ˆ ë§ì”€í•´ì£¼ì„¸ìš”~"
    }
    return feedbacks.get(command_type, "ëª…ë ¹ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.")

def play_audio_with_pygame(file_path):
    """pygameìœ¼ë¡œ ì˜¤ë””ì˜¤ ìž¬ìƒ"""
    pygame.mixer.init()
    pygame.mixer.music.load(file_path)
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        pygame.time.Clock().tick(10)

async def tts_and_play(text, voice="ko-KR-JiMinNeural", filename="output.mp3"):
    """TTS ìŒì„± í•©ì„± ë° ìž¬ìƒ"""
    tts = edge_tts.Communicate(
        text,
        voice=voice,
        rate="+10%",
        pitch="+5Hz"
    )
    await tts.save(filename)
    play_audio_with_pygame(filename)

def extract_entities(tokens, ner_tags):
    """í† í°ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
    entities = []
    current_entity = None
    
    for token, tag in zip(tokens, ner_tags):
        # [CLS], [SEP], [PAD] ë“± íŠ¹ìˆ˜ í† í° ì œì™¸
        if token in ['[CLS]', '[SEP]', '[PAD]']:
            continue
            
        if tag.startswith("B-"):
            if current_entity:
                entities.append(current_entity)
            current_entity = {
                "text": token.replace("##", ""), 
                "type": tag[2:]
            }
        elif tag.startswith("I-") and current_entity:
            current_entity["text"] += token.replace("##", "")
        else:
            if current_entity:
                entities.append(current_entity)
                current_entity = None
    
    if current_entity:
        entities.append(current_entity)
    
    return entities

# Intent ë§¤í•‘
INTENT_TO_ACTION = {
    'SELECT_BY_ARTIST_TITLE': 'PLAY_MUSIC',
    'SELECT_BY_ARTIST': 'PLAY_MUSIC',
    'SELECT_BY_TITLE': 'PLAY_MUSIC',
    'NEXT_SONG': 'NEXT_SONG',
    'PAUSE': 'PAUSE',
    'RESUME': 'RESUME',
    'STOP': 'STOP_ALL',
    'START_LISTENING': 'START_LISTENING',
    'START_EXERCISE': 'START_EXERCISE',
    'SWITCH_TO_EXERCISE': 'SWITCH_TO_EXERCISE',
    'SWITCH_TO_LISTENING': 'SWITCH_TO_LISTENING',
    'EMERGENCY': 'EMERGENCY',
    'NONE': 'UNKNOWN'
}

# ============================================
# ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ============================================
print("\n" + "=" * 60)
print("ðŸŽ¤ ìŒì„± ëª…ë ¹ ì²˜ë¦¬ ì‹œìž‘")
print("=" * 60)

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œìž‘ ì‹œê°„
pipeline_start = time.time()

audio_path = r"C:\Users\SSAFY\Documents\ì†Œë¦¬ ë…¹ìŒ\test5_voice.m4a"
processed_path = audio_path.replace(".m4a", "_processed.wav")

print(f"\nðŸ“‚ ì˜¤ë””ì˜¤ íŒŒì¼: {os.path.basename(audio_path)}")

# --- 0-1. ì „ì²˜ë¦¬ ì „ ìŒì„± ì¸ì‹ (ë¹„êµìš©) ---
print("\n" + "=" * 60)
print("ðŸ“Š [ë¹„êµ] ì „ì²˜ë¦¬ ì „ ìŒì„± ì¸ì‹")
print("=" * 60)

stt_before_start = time.time()
result_before = model_whisper.transcribe(audio_path)
text_before = result_before['text']
stt_before_time = time.time() - stt_before_start

print(f"âœ“ ì¸ì‹ ì™„ë£Œ ({stt_before_time:.2f}ì´ˆ)")
print(f"ðŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: '{text_before}'")

# --- 0-2. ìŒì„± ì „ì²˜ë¦¬ ---
print("\n" + "=" * 60)
print("ðŸŽ™ï¸  ìŒì„± ì „ì²˜ë¦¬ ì ìš©")
print("=" * 60)

processed_audio, sr, preprocess_time = preprocessor.preprocess(
    file_path=audio_path,
    output_path=processed_path
)

# --- 1. ì „ì²˜ë¦¬ í›„ ìŒì„± ì¸ì‹ (STT) ---
print("\n" + "=" * 60)
print("ðŸ“Š ì „ì²˜ë¦¬ í›„ ìŒì„± ì¸ì‹")
print("=" * 60)

stt_start = time.time()
result = model_whisper.transcribe(processed_path)  # ì „ì²˜ë¦¬ëœ íŒŒì¼ ì‚¬ìš©
input_text = result['text']
stt_time = time.time() - stt_start

print(f"âœ“ ìŒì„± ì¸ì‹ ì™„ë£Œ ({stt_time:.2f}ì´ˆ)")
print(f"ðŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: '{input_text}'")

# --- ì „ì²˜ë¦¬ íš¨ê³¼ ë¹„êµ ---
print("\n" + "=" * 60)
print("ðŸ“ˆ ì „ì²˜ë¦¬ íš¨ê³¼ ë¹„êµ")
print("=" * 60)
print(f"\nðŸ”´ ì „ì²˜ë¦¬ ì „:")
print(f"   \"{text_before}\"")
print(f"\nðŸŸ¢ ì „ì²˜ë¦¬ í›„:")
print(f"   \"{input_text}\"")

# í…ìŠ¤íŠ¸ ë³€í™” ë¶„ì„
if text_before != input_text:
    print(f"\nâœ¨ ë³€í™” ê°ì§€!")
    # ê¸€ìž ìˆ˜ ë¹„êµ
    print(f"   ê¸¸ì´: {len(text_before)} â†’ {len(input_text)} ê¸€ìž")
    # ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë°©ë²•)
    common_chars = sum(1 for a, b in zip(text_before, input_text) if a == b)
    if len(text_before) > 0:
        similarity = common_chars / max(len(text_before), len(input_text)) * 100
        print(f"   ìœ ì‚¬ë„: {similarity:.1f}%")
else:
    print(f"\n   (ë™ì¼í•œ ê²°ê³¼)")

print("=" * 60)

# --- 2. NLU (Intent & NER) ---
print(f"\nðŸ§  ì˜ë„ ë¶„ì„ ì¤‘...")
nlu_start = time.time()

inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

with torch.no_grad():
    outputs = model_ner_intent(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"]
    )

# Intent ì˜ˆì¸¡
intent_logits = outputs["intent_logits"][0]
intent_probs = torch.softmax(intent_logits, dim=-1)
intent_pred = torch.argmax(intent_logits).item()
intent_label = id2intent[intent_pred]
intent_confidence = intent_probs[intent_pred].item()

# NER ì˜ˆì¸¡
ner_preds = torch.argmax(outputs["ner_logits"], dim=-1)[0].tolist()
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
ner_tags = [ner_id2tag.get(p, "O") for p in ner_preds]

# ì—”í‹°í‹° ì¶”ì¶œ
entities = extract_entities(tokens, ner_tags)

nlu_time = time.time() - nlu_start

print(f"âœ“ ì˜ë„ ë¶„ì„ ì™„ë£Œ ({nlu_time:.4f}ì´ˆ)")

# --- 3. ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê¹… ì •ë³´) ---
print("\n" + "=" * 60)
print("ðŸ“Š ë¶„ì„ ê²°ê³¼")
print("=" * 60)

print(f"\nðŸŽ¯ Intent ì˜ˆì¸¡:")
print(f"   â–¶ {intent_label} (í™•ì‹ ë„: {intent_confidence:.1%})")

# Top-3 í›„ë³´ í‘œì‹œ
top3 = torch.topk(intent_probs, k=min(3, len(intent_probs)))
print(f"\n   Top-3 í›„ë³´:")
for idx, prob in zip(top3.indices, top3.values):
    intent_name = id2intent[idx.item()]
    is_predicted = "âœ“" if idx.item() == intent_pred else " "
    print(f"   {is_predicted} {intent_name:25} {prob.item():.1%}")

# ì—”í‹°í‹° ì¶œë ¥
print(f"\nðŸ·ï¸  ì¶”ì¶œëœ ì—”í‹°í‹° ({len(entities)}ê°œ):")
if entities:
    for ent in entities:
        print(f"   [{ent['type']:6}] {ent['text']}")
else:
    print(f"   (ì—†ìŒ)")

# í† í°ë³„ NER íƒœê·¸ (ìƒì„¸)
print(f"\nðŸ“ í† í°ë³„ NER íƒœê¹…:")
for token, tag in zip(tokens, ner_tags):
    if token not in ['[CLS]', '[SEP]', '[PAD]']:
        print(f"   {token:15} â†’ {tag}")

# --- 4. ì‘ë‹µ ìƒì„± ë° TTS ---
command_type = INTENT_TO_ACTION.get(intent_label, 'UNKNOWN')
feedback_text = generate_feedback(command_type)

print(f"\nðŸ’¬ ì‹œìŠ¤í…œ ì‘ë‹µ: '{feedback_text}'")
print(f"ðŸ”Š ìŒì„± í•©ì„± ë° ìž¬ìƒ ì¤‘...")

tts_start = time.time()
asyncio.run(tts_and_play(feedback_text))
tts_time = time.time() - tts_start

print(f"âœ“ ì‘ë‹µ ì™„ë£Œ ({tts_time:.2f}ì´ˆ)")

# --- 5. ìŒì•… ìž¬ìƒ (PLAY_MUSICì¸ ê²½ìš°) ---
if command_type == 'PLAY_MUSIC':
    music_file_path = r"C:\Users\SSAFY\í¥ë¶€ìž\S13P31A103\mockup\BE\AI_ë‚˜ì´ê°€ ì–´ë•Œì„œ.mp3"
    
    artist = next((e['text'] for e in entities if e['type'] == 'ARTIST'), None)
    song = next((e['text'] for e in entities if e['type'] == 'SONG'), None)
    
    print(f"\nðŸŽµ ìŒì•… ìž¬ìƒ:")
    if artist or song:
        print(f"   ìš”ì²­: {artist or '(ë¯¸ì§€ì •)'} - {song or '(ë¯¸ì§€ì •)'}")
    print(f"   íŒŒì¼: {os.path.basename(music_file_path)}")
    
    play_audio_with_pygame(music_file_path)

# ============================================
# ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
# ============================================
total_time = time.time() - pipeline_start

print("\n" + "=" * 60)
print("â±ï¸  ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼")
print("=" * 60)
print(f"   STT (ì „ì²˜ë¦¬ ì „):    {stt_before_time:6.2f}ì´ˆ")
print(f"   ìŒì„± ì „ì²˜ë¦¬:        {preprocess_time:6.2f}ì´ˆ  ({preprocess_time/total_time*100:5.1f}%)")
print(f"   STT (ì „ì²˜ë¦¬ í›„):    {stt_time:6.2f}ì´ˆ  ({stt_time/total_time*100:5.1f}%)")
print(f"   NLU (Intent+NER):   {nlu_time:6.4f}ì´ˆ  ({nlu_time/total_time*100:5.1f}%)")
print(f"   TTS (ì‘ë‹µ ìƒì„±):    {tts_time:6.2f}ì´ˆ  ({tts_time/total_time*100:5.1f}%)")
print("   " + "-" * 56)
print(f"   ì´ ì†Œìš” ì‹œê°„:       {total_time:6.2f}ì´ˆ")

# ì „ì²˜ë¦¬ íš¨ê³¼ ìš”ì•½
print("\n" + "=" * 60)
print("ðŸ“Š ì „ì²˜ë¦¬ íš¨ê³¼ ìš”ì•½")
print("=" * 60)
print(f"   ì¸ì‹ í…ìŠ¤íŠ¸ ë³€í™”: {'ìžˆìŒ âœ“' if text_before != input_text else 'ì—†ìŒ'}")
if text_before != input_text:
    print(f"   ì „: \"{text_before[:50]}{'...' if len(text_before) > 50 else ''}\"")
    print(f"   í›„: \"{input_text[:50]}{'...' if len(input_text) > 50 else ''}\"")
print(f"   ì „ì²˜ë¦¬ ì‹œê°„ ë¹„ìš©: {preprocess_time:.2f}ì´ˆ")
print("=" * 60)

print("\nâœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")