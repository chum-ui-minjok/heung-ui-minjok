"""
ì‹¤ì‹œê°„ ì›¹ìº  ë™ì‘ ì¸ì‹ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python realtime_webcam_test.py

ì˜µì…˜:
    --camera: ì¹´ë©”ë¼ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)
    --action-code: í…ŒìŠ¤íŠ¸í•  ë™ì‘ ì½”ë“œ (ê¸°ë³¸ê°’: 1, ì† ë°•ìˆ˜)
    --action-name: í…ŒìŠ¤íŠ¸í•  ë™ì‘ ì´ë¦„ (ê¸°ë³¸ê°’: ì† ë°•ìˆ˜)
    --fps: ìº¡ì²˜ FPS (ê¸°ë³¸ê°’: 10)

í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤:
    SPACE: í”„ë ˆì„ ìˆ˜ì§‘ ì‹œì‘/ì¤‘ì§€
    Q: ì¢…ë£Œ
    1-7: ë™ì‘ ë³€ê²½ (1=ì†ë°•ìˆ˜, 2=íŒ”ì¹˜ê¸°, 4=íŒ”ë»—ê¸°, 5=ê¸°ìš°ëš±, 6=ë¹„ìƒêµ¬, 7=ê²¨ë“œë‘ì´, 9=ê°€ë§Œíˆ)
"""

import argparse
import base64
import io
import sys
import time
from collections import deque
from pathlib import Path
from typing import Deque, Optional

import cv2
import numpy as np
from PIL import Image

# Motion server íŒ¨í‚¤ì§€ import
sys.path.insert(0, str(Path(__file__).parent))
from app.services.inference import get_inference_service, InferenceResult


class RealtimeMotionTester:
    """ì‹¤ì‹œê°„ ì›¹ìº  ë™ì‘ ì¸ì‹ í…ŒìŠ¤í„°"""

    # ë™ì‘ ì½”ë“œ â†’ ì´ë¦„ ë§¤í•‘
    ACTION_NAMES = {
        1: "ì† ë°•ìˆ˜",
        2: "íŒ” ì¹˜ê¸°",
        4: "íŒ” ë»—ê¸°",
        5: "ê¸°ìš°ëš±",
        6: "ë¹„ìƒêµ¬",
        7: "ê²¨ë“œë‘ì´ë°•ìˆ˜",
        9: "ê°€ë§Œíˆ ìˆìŒ",
    }

    def __init__(
        self,
        camera_index: int = 0,
        target_action_code: int = 1,
        capture_fps: int = 10,
        frames_per_sample: int = 8,
    ):
        """
        Args:
            camera_index: ì›¹ìº  ì¸ë±ìŠ¤
            target_action_code: í…ŒìŠ¤íŠ¸í•  ë™ì‘ ì½”ë“œ
            capture_fps: í”„ë ˆì„ ìº¡ì²˜ FPS
            frames_per_sample: AI ë¶„ì„ì— ì‚¬ìš©í•  í”„ë ˆì„ ìˆ˜
        """
        self.camera_index = camera_index
        self.target_action_code = target_action_code
        self.target_action_name = self.ACTION_NAMES.get(target_action_code, "ì•Œ ìˆ˜ ì—†ìŒ")
        self.capture_fps = capture_fps
        self.frames_per_sample = frames_per_sample

        # ì›¹ìº  ì´ˆê¸°í™”
        self.cap = cv2.VideoCapture(camera_index)
        if not self.cap.isOpened():
            raise RuntimeError(f"ì¹´ë©”ë¼ {camera_index}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # í•´ìƒë„ ì„¤ì • (640x480)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        # AI ëª¨ë¸ ë¡œë“œ
        print("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.inference_service = get_inference_service()
        print("âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

        # í”„ë ˆì„ ë²„í¼ (ìµœê·¼ Nê°œ í”„ë ˆì„ ì €ì¥)
        self.frame_buffer: Deque[str] = deque(maxlen=frames_per_sample)

        # ìƒíƒœ ë³€ìˆ˜
        self.is_collecting = False  # í”„ë ˆì„ ìˆ˜ì§‘ ì¤‘ ì—¬ë¶€
        self.last_result: Optional[InferenceResult] = None
        self.last_inference_time = 0

    def run(self):
        """ë©”ì¸ ë£¨í”„"""
        print("\n" + "=" * 80)
        print("ğŸ¥ ì‹¤ì‹œê°„ ì›¹ìº  ë™ì‘ ì¸ì‹ ì‹œì‘!")
        print("=" * 80)
        print(f"ğŸ“¹ ì¹´ë©”ë¼: {self.camera_index}")
        print(f"ğŸ¯ ëª©í‘œ ë™ì‘: {self.target_action_name} (ì½”ë“œ: {self.target_action_code})")
        print(f"â±ï¸ ìº¡ì²˜ FPS: {self.capture_fps}")
        print(f"ğŸ“¦ ìƒ˜í”Œ í”„ë ˆì„ ìˆ˜: {self.frames_per_sample}")
        print("\ní‚¤ë³´ë“œ ë‹¨ì¶•í‚¤:")
        print("  - SPACE: í”„ë ˆì„ ìˆ˜ì§‘ ì‹œì‘/ì¤‘ì§€")
        print("  - Q: ì¢…ë£Œ")
        print("  - 1-7: ë™ì‘ ë³€ê²½")
        print("=" * 80 + "\n")

        frame_interval = 1.0 / self.capture_fps
        last_frame_time = 0

        try:
            while True:
                ret, frame = self.cap.read()
                if not ret:
                    print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    break

                # ì¢Œìš° ë°˜ì „ (ì…€ì¹´ ëª¨ë“œ)
                frame = cv2.flip(frame, 1)

                current_time = time.time()

                # FPS ì œì–´: ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ í”„ë ˆì„ ìˆ˜ì§‘
                if self.is_collecting and (current_time - last_frame_time) >= frame_interval:
                    self._collect_frame(frame)
                    last_frame_time = current_time

                    # ë²„í¼ê°€ ë‹¤ ì°¼ìœ¼ë©´ AI ë¶„ì„ ì‹¤í–‰
                    if len(self.frame_buffer) == self.frames_per_sample:
                        self._run_inference()
                        self.frame_buffer.clear()

                # UI ê·¸ë¦¬ê¸°
                self._draw_ui(frame)

                # í™”ë©´ í‘œì‹œ
                cv2.imshow("ì‹¤ì‹œê°„ ë™ì‘ ì¸ì‹", frame)

                # í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == ord("q"):
                    print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                    break
                elif key == ord(" "):
                    self.is_collecting = not self.is_collecting
                    if self.is_collecting:
                        print(f"\nâ–¶ï¸ í”„ë ˆì„ ìˆ˜ì§‘ ì‹œì‘! ({self.frames_per_sample}ê°œ)")
                        self.frame_buffer.clear()
                        self.last_result = None
                    else:
                        print("\nâ¸ï¸ í”„ë ˆì„ ìˆ˜ì§‘ ì¤‘ì§€")
                elif key in [ord("1"), ord("2"), ord("4"), ord("5"), ord("6"), ord("7"), ord("9")]:
                    code = int(chr(key))
                    if code in self.ACTION_NAMES:
                        self._change_target_action(code)

        finally:
            self.cap.release()
            cv2.destroyAllWindows()

    def _collect_frame(self, frame: np.ndarray):
        """í”„ë ˆì„ì„ Base64ë¡œ ë³€í™˜í•˜ì—¬ ë²„í¼ì— ì¶”ê°€"""
        # OpenCV BGR â†’ PIL RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)

        # JPEGë¡œ ì¸ì½”ë”© í›„ Base64 ë³€í™˜
        buffer = io.BytesIO()
        pil_image.save(buffer, format="JPEG", quality=85)
        buffer.seek(0)
        base64_str = base64.b64encode(buffer.read()).decode("utf-8")

        self.frame_buffer.append(base64_str)

    def _run_inference(self):
        """ìˆ˜ì§‘ëœ í”„ë ˆì„ìœ¼ë¡œ AI ì¶”ë¡  ì‹¤í–‰"""
        start_time = time.time()

        try:
            print(f"\nğŸ” AI ë¶„ì„ ì‹œì‘... (í”„ë ˆì„: {len(self.frame_buffer)}ê°œ)")

            result = self.inference_service.predict(
                frames=list(self.frame_buffer),
                target_action_name=self.target_action_name,
                target_action_code=self.target_action_code,
            )

            self.last_result = result
            self.last_inference_time = (time.time() - start_time) * 1000

            # ê²°ê³¼ ì¶œë ¥
            self._print_result(result)

        except ValueError as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.last_result = None
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
            self.last_result = None

    def _print_result(self, result: InferenceResult):
        """ì¶”ë¡  ê²°ê³¼ ì½˜ì†” ì¶œë ¥"""
        score_emoji = ["âŒ", "âš ï¸", "âœ…", "ğŸ¯"]

        print("\n" + "=" * 80)
        print("ğŸ¯ AI íŒì • ê²°ê³¼")
        print("=" * 80)
        print(f"  ëª©í‘œ ë™ì‘: {self.target_action_name} (ì½”ë“œ: {self.target_action_code})")
        print(f"  ì˜ˆì¸¡ ë™ì‘: {result.predicted_label}")
        print(f"  ì˜ˆì¸¡ ì‹ ë¢°ë„: {result.confidence * 100:.1f}%")

        if result.target_probability is not None:
            print(f"  ëª©í‘œ í™•ë¥ : {result.target_probability * 100:.1f}%")

        print(f"\n  ìµœì¢… ì ìˆ˜: {result.judgment}ì  {score_emoji[result.judgment]}")

        total_ms = result.decode_time_ms + result.pose_time_ms + result.inference_time_ms
        print(f"\n  ì²˜ë¦¬ ì‹œê°„: {total_ms:.0f}ms (ì¶”ë¡ : {result.inference_time_ms:.0f}ms)")
        print("=" * 80 + "\n")

    def _draw_ui(self, frame: np.ndarray):
        """í™”ë©´ì— UI ê·¸ë¦¬ê¸°"""
        height, width = frame.shape[:2]

        # ë°˜íˆ¬ëª… ë°°ê²½
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (width - 10, 200), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)

        # í…ìŠ¤íŠ¸ ì •ë³´
        y_offset = 40
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.7
        thickness = 2

        # ìˆ˜ì§‘ ìƒíƒœ
        status_text = "ìˆ˜ì§‘ ì¤‘..." if self.is_collecting else "ëŒ€ê¸° ì¤‘ (SPACEë¡œ ì‹œì‘)"
        status_color = (0, 255, 0) if self.is_collecting else (100, 100, 100)
        cv2.putText(frame, status_text, (20, y_offset), font, font_scale, status_color, thickness)
        y_offset += 35

        # ëª©í‘œ ë™ì‘
        cv2.putText(
            frame,
            f"ëª©í‘œ: {self.target_action_name} (ì½”ë“œ: {self.target_action_code})",
            (20, y_offset),
            font,
            0.6,
            (255, 255, 255),
            1,
        )
        y_offset += 30

        # ë²„í¼ ìƒíƒœ
        buffer_text = f"ë²„í¼: {len(self.frame_buffer)}/{self.frames_per_sample}"
        cv2.putText(frame, buffer_text, (20, y_offset), font, 0.6, (255, 255, 255), 1)
        y_offset += 35

        # ë§ˆì§€ë§‰ ê²°ê³¼ í‘œì‹œ
        if self.last_result:
            # íŒì • ê²°ê³¼
            judgment_color = self._get_judgment_color(self.last_result.judgment)
            judgment_text = f"íŒì •: {self.last_result.judgment}ì "
            cv2.putText(frame, judgment_text, (20, y_offset), font, font_scale, judgment_color, thickness)
            y_offset += 30

            # ì˜ˆì¸¡ ë™ì‘
            predicted_text = f"ì˜ˆì¸¡: {self.last_result.predicted_label}"
            cv2.putText(frame, predicted_text, (20, y_offset), font, 0.6, (255, 255, 255), 1)
            y_offset += 25

            # ì‹ ë¢°ë„
            confidence_text = f"ì‹ ë¢°ë„: {self.last_result.confidence * 100:.1f}%"
            cv2.putText(frame, confidence_text, (20, y_offset), font, 0.6, (255, 255, 255), 1)
            y_offset += 25

            # ëª©í‘œ í™•ë¥ 
            if self.last_result.target_probability is not None:
                target_prob_text = f"ëª©í‘œí™•ë¥ : {self.last_result.target_probability * 100:.1f}%"
                cv2.putText(frame, target_prob_text, (20, y_offset), font, 0.6, (255, 255, 255), 1)

        # í•˜ë‹¨ ë„ì›€ë§
        help_text = "Q: ì¢…ë£Œ | SPACE: ì‹œì‘/ì¤‘ì§€ | 1-9: ë™ì‘ë³€ê²½"
        cv2.putText(
            frame,
            help_text,
            (20, height - 20),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (200, 200, 200),
            1,
        )

    def _get_judgment_color(self, judgment: int) -> tuple:
        """íŒì • ì ìˆ˜ì— ë”°ë¥¸ ìƒ‰ìƒ ë°˜í™˜ (BGR)"""
        colors = {
            0: (0, 0, 255),  # ë¹¨ê°•
            1: (0, 165, 255),  # ì£¼í™©
            2: (0, 255, 255),  # ë…¸ë‘
            3: (0, 255, 0),  # ì´ˆë¡
        }
        return colors.get(judgment, (255, 255, 255))

    def _change_target_action(self, action_code: int):
        """ëª©í‘œ ë™ì‘ ë³€ê²½"""
        self.target_action_code = action_code
        self.target_action_name = self.ACTION_NAMES[action_code]
        self.last_result = None
        print(f"\nğŸ¯ ëª©í‘œ ë™ì‘ ë³€ê²½: {self.target_action_name} (ì½”ë“œ: {action_code})")


def main():
    parser = argparse.ArgumentParser(description="ì‹¤ì‹œê°„ ì›¹ìº  ë™ì‘ ì¸ì‹ í…ŒìŠ¤íŠ¸")
    parser.add_argument(
        "--camera",
        type=int,
        default=0,
        help="ì¹´ë©”ë¼ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)",
    )
    parser.add_argument(
        "--action-code",
        type=int,
        default=1,
        help="í…ŒìŠ¤íŠ¸í•  ë™ì‘ ì½”ë“œ (ê¸°ë³¸ê°’: 1, ì† ë°•ìˆ˜)",
    )
    parser.add_argument(
        "--fps",
        type=int,
        default=10,
        help="ìº¡ì²˜ FPS (ê¸°ë³¸ê°’: 10)",
    )

    args = parser.parse_args()

    try:
        tester = RealtimeMotionTester(
            camera_index=args.camera,
            target_action_code=args.action_code,
            capture_fps=args.fps,
        )
        tester.run()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
