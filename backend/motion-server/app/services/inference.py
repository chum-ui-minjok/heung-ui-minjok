"""Î™®ÏÖò Ï∂îÎ°† Î™®Îç∏ Î∞è Ï†ÑÏ≤òÎ¶¨ Ïú†Ìã∏Î¶¨Ìã∞."""

from __future__ import annotations

import base64
import logging
import os
from dataclasses import dataclass
from functools import lru_cache
from io import BytesIO
from pathlib import Path
from time import perf_counter
from typing import Iterable, List, Sequence, Tuple

import mediapipe as mp
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image

LOGGER = logging.getLogger(__name__)


@dataclass
class InferenceResult:
    predicted_label: str
    confidence: float
    judgment: int
    decode_time_ms: float
    pose_time_ms: float
    inference_time_ms: float
    action_code: int | None
    target_probability: float | None = None


class GraphConvLayer(nn.Module):
    """Í∞ÑÎã®Ìïú GCN Î†àÏù¥Ïñ¥."""

    def __init__(self, num_nodes: int, in_features: int, out_features: int) -> None:
        super().__init__()
        self.register_parameter(
            "adjacency", nn.Parameter(torch.eye(num_nodes, dtype=torch.float32))
        )
        self.linear = nn.Linear(in_features, out_features)
        self.norm = nn.LayerNorm(out_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, T, N, F)
        adjacency = torch.softmax(self.adjacency, dim=-1)
        aggregated = torch.einsum("nj,btjf->btnf", adjacency, x)
        out = self.linear(aggregated)
        out = self.norm(out)
        return F.relu(out)


class TemporalCNN(nn.Module):
    """ÏãúÍ≥ÑÏó¥ Ìå®ÌÑ¥ Ï∂îÏ∂úÏö© 1D CNN Î∏îÎ°ù."""

    def __init__(
        self,
        in_channels: int,
        channel_sizes: Sequence[int],
        dropout: float,
    ) -> None:
        super().__init__()
        layers: List[nn.Module] = []
        input_channels = in_channels
        for out_channels in channel_sizes:
            layers.append(nn.Conv1d(input_channels, out_channels, kernel_size=3, padding=1))
            layers.append(nn.BatchNorm1d(out_channels))
            layers.append(nn.ReLU(inplace=True))
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            input_channels = out_channels
        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


class MotionGCNCNN(nn.Module):
    """GCN + Temporal CNN Í∏∞Î∞ò Î™®ÏÖò Î∂ÑÎ•òÍ∏∞."""

    def __init__(
        self,
        num_nodes: int,
        input_dim: int,
        gcn_hidden_dims: Sequence[int],
        temporal_channels: Sequence[int],
        num_classes: int,
        dropout: float,
    ) -> None:
        super().__init__()
        self.gcn_layers = nn.ModuleList()
        prev_dim = input_dim
        for hidden_dim in gcn_hidden_dims:
            self.gcn_layers.append(GraphConvLayer(num_nodes, prev_dim, hidden_dim))
            prev_dim = hidden_dim

        self.temporal_cnn = TemporalCNN(prev_dim, temporal_channels, dropout)
        self.temporal_pool = nn.AdaptiveAvgPool1d(1)
        classifier_layers: List[nn.Module] = [
            nn.Linear(temporal_channels[-1], temporal_channels[-1]),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(temporal_channels[-1], num_classes),
        ]
        self.classifier = nn.Sequential(*classifier_layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, T, N, F)
        for layer in self.gcn_layers:
            x = layer(x)

        # ÎÖ∏Îìú ÌèâÍ∑†ÏúºÎ°ú Í∑∏ÎûòÌîÑ ÏûÑÎ≤†Îî© ÏÉùÏÑ±
        x = torch.mean(x, dim=2)  # (B, T, H)
        x = x.permute(0, 2, 1)  # (B, H, T)
        x = self.temporal_cnn(x)
        x = self.temporal_pool(x).squeeze(-1)
        logits = self.classifier(x)
        return logits


class PoseExtractor:
    """Mediapipe PoseÎ•º ÌôúÏö©Ìïú 2D Í¥ÄÏ†à Ï¢åÌëú Ï∂îÏ∂úÍ∏∞."""

    # ÏÑ†ÌÉùÌïú ÎûúÎìúÎßàÌÅ¨ Ïù∏Îç±Ïä§ (Ï¥ù 22Í∞ú)
    SELECTED_LANDMARKS = [
        mp.solutions.pose.PoseLandmark.LEFT_SHOULDER,
        mp.solutions.pose.PoseLandmark.LEFT_ELBOW,
        mp.solutions.pose.PoseLandmark.LEFT_WRIST,
        mp.solutions.pose.PoseLandmark.LEFT_THUMB,
        mp.solutions.pose.PoseLandmark.LEFT_PINKY,
        mp.solutions.pose.PoseLandmark.LEFT_HIP,
        mp.solutions.pose.PoseLandmark.LEFT_KNEE,
        mp.solutions.pose.PoseLandmark.LEFT_ANKLE,
        mp.solutions.pose.PoseLandmark.LEFT_HEEL,
        mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX,
        mp.solutions.pose.PoseLandmark.MOUTH_LEFT,
        mp.solutions.pose.PoseLandmark.NOSE,
        mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER,
        mp.solutions.pose.PoseLandmark.RIGHT_ELBOW,
        mp.solutions.pose.PoseLandmark.RIGHT_WRIST,
        mp.solutions.pose.PoseLandmark.RIGHT_THUMB,
        mp.solutions.pose.PoseLandmark.RIGHT_PINKY,
        mp.solutions.pose.PoseLandmark.RIGHT_HIP,
        mp.solutions.pose.PoseLandmark.RIGHT_KNEE,
        mp.solutions.pose.PoseLandmark.RIGHT_ANKLE,
        mp.solutions.pose.PoseLandmark.RIGHT_HEEL,
        mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX,
    ]

    ROOT_INDICES = [
        SELECTED_LANDMARKS.index(mp.solutions.pose.PoseLandmark.LEFT_HIP),
        SELECTED_LANDMARKS.index(mp.solutions.pose.PoseLandmark.RIGHT_HIP),
    ]

    def __init__(self) -> None:
        self._pose = mp.solutions.pose.Pose(
            static_image_mode=True,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.5,
        )

    def extract(self, image: np.ndarray) -> np.ndarray:
        results = self._pose.process(image)
        if not results.pose_landmarks:
            return np.zeros((len(self.SELECTED_LANDMARKS), 2), dtype=np.float32)

        landmarks = results.pose_landmarks.landmark
        coords = []
        for idx in self.SELECTED_LANDMARKS:
            lm = landmarks[idx]
            coords.append((lm.x, lm.y))
        keypoints = np.array(coords, dtype=np.float32)
        return self._normalize(keypoints)

    def _normalize(self, keypoints: np.ndarray) -> np.ndarray:
        if not np.any(keypoints):
            return keypoints

        valid_points = keypoints[self.ROOT_INDICES]
        if np.all(valid_points == 0):
            center = np.mean(keypoints, axis=0)
        else:
            center = np.mean(valid_points, axis=0)

        keypoints -= center

        max_distance = np.linalg.norm(keypoints, axis=1).max()
        if max_distance > 0:
            keypoints /= max_distance

        return keypoints


class MotionInferenceService:
    """ÌïôÏäµÎêú Î™®Îç∏ÏùÑ Î°úÎìúÌïòÏó¨ ÌîÑÎ†àÏûÑ ÏãúÌÄÄÏä§Î•º ÌåêÏ†ï Ï†êÏàòÎ°ú Î≥ÄÌôò."""

    def __init__(self, model_path: Path, device: str | None = None) -> None:
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)
        args = checkpoint.get("args", {})
        class_mapping = checkpoint.get("class_mapping", {})

        if device:
            requested = torch.device(device)
            if requested.type == "cuda" and not torch.cuda.is_available():
                raise RuntimeError("CUDA Ïû•ÏπòÍ∞Ä ÏöîÏ≤≠ÎêòÏóàÏßÄÎßå ÏÇ¨Ïö© Í∞ÄÎä•ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
            self.device = requested
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        LOGGER.info("Motion inference device: %s", self.device)
        self.frames_per_sample = int(args.get("frames_per_sample", 8))
        self.class_mapping = {label.upper(): index for label, index in class_mapping.items()}
        self.id_to_label = {index: label for label, index in self.class_mapping.items()}

        gcn_hidden_dims = args.get("gcn_hidden_dims", [64, 128])
        temporal_channels = args.get("temporal_channels", [128, 256])
        dropout = float(args.get("dropout", 0.3))

        self.model = MotionGCNCNN(
            num_nodes=checkpoint["model_state_dict"]["gcn_layers.0.adjacency"].shape[0],
            input_dim=checkpoint["model_state_dict"]["gcn_layers.0.linear.weight"].shape[1],
            gcn_hidden_dims=gcn_hidden_dims,
            temporal_channels=temporal_channels,
            num_classes=len(self.class_mapping),
            dropout=dropout,
        )
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.model.to(self.device)
        self.model.eval()

        self.pose_extractor = PoseExtractor()

    def predict(
        self,
        frames: Sequence[str],
        target_action_name: str | None = None,
        target_action_code: int | None = None,
    ) -> InferenceResult:
        if not frames:
            raise ValueError("ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§.")

        sampled_frames = self._sample_frames(frames, self.frames_per_sample)
        keypoint_sequence, decode_time_s, pose_time_s = self._frames_to_keypoints(
            sampled_frames
        )

        input_tensor = torch.from_numpy(keypoint_sequence).unsqueeze(0)  # (1, T, N, 2)
        input_tensor = input_tensor.to(self.device)

        with torch.no_grad():
            inference_start = perf_counter()
            logits = self.model(input_tensor)
            inference_time_ms = (perf_counter() - inference_start) * 1000
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

        decode_time_ms = decode_time_s * 1000
        pose_time_ms = pose_time_s * 1000

        best_idx = int(np.argmax(probabilities))
        predicted_label = self.id_to_label.get(best_idx, "UNKNOWN")
        confidence = float(probabilities[best_idx])

        target_index = self._resolve_target_index(target_action_name, target_action_code)
        target_probability: float | None = None
        if target_index is not None and 0 <= target_index < len(probabilities):
            target_probability = float(probabilities[target_index])
            judgment = self._score_by_probability(target_probability)
        else:
            judgment = self._fallback_score(predicted_label, confidence, target_action_name)

        total_time_ms = decode_time_ms + pose_time_ms + inference_time_ms
        LOGGER.info(
            "üéØ AI ÌåêÏ†ï Í≤∞Í≥º - Î™©ÌëúÎèôÏûë=%s(code=%s), ÏòàÏ∏°ÎèôÏûë=%s(Ïã†Î¢∞ÎèÑ=%.1f%%), "
            "Î™©ÌëúÌôïÎ•†=%.1f%%, Ï†êÏàò=%dÏ†ê | ‚è±Ô∏è Ï¥ù=%.0fms (ÎîîÏΩîÎî©=%.0fms, PoseÏ∂îÏ∂ú=%.0fms, Ï∂îÎ°†=%.0fms)",
            target_action_name,
            target_action_code,
            predicted_label,
            confidence * 100,
            (target_probability * 100) if target_probability else 0,
            judgment,
            total_time_ms,
            decode_time_ms,
            pose_time_ms,
            inference_time_ms,
        )

        # ========================================================================
        # ‚ö†Ô∏è CRITICAL: Convert model index back to DB actionCode when returning
        # ========================================================================
        # If target_action_code was provided, return it as-is (already DB format)
        # If not provided, convert model's best_idx (0-based) to DB actionCode (1-based)
        # ========================================================================
        resolved_action_code = (
            target_action_code if target_action_code is not None else best_idx + 1
        )

        return InferenceResult(
            predicted_label=predicted_label,
            confidence=confidence,
            judgment=judgment,
            action_code=resolved_action_code,
            decode_time_ms=decode_time_ms,
            pose_time_ms=pose_time_ms,
            inference_time_ms=inference_time_ms,
            target_probability=target_probability,
        )

    def _resolve_target_index(
        self, action_name: str | None, action_code: int | None
    ) -> int | None:
        # ========================================================================
        # ‚ö†Ô∏è CRITICAL: DB actionCode vs Model class_index Mismatch Fix
        # ========================================================================
        # - DB actionCode is 1-based (ÏÜê Î∞ïÏàò=1, Ìåî ÏπòÍ∏∞=2, ...)
        # - AI model class_index is 0-based (ÏÜê Î∞ïÏàò=0, Ìåî ÏπòÍ∏∞=1, ...)
        # - This causes 1-position shift ‚Üí incorrect predictions!
        #
        # TEMPORARY FIX: Convert DB actionCode to model index by subtracting 1
        # TODO: Retrain model with 1-based labels OR update DB to use 0-based codes
        # ========================================================================
        if action_code is not None:
            # Convert DB actionCode (1-based) to model class_index (0-based)
            model_index = action_code - 1
            if model_index in self.id_to_label:
                return model_index
        if action_name:
            key = action_name.strip().upper()
            return self.class_mapping.get(key)
        return None

    @staticmethod
    def _score_by_probability(probability: float) -> int:
        # ========================================================================
        # ‚ö†Ô∏è CRITICAL: Stricter scoring criteria to prevent high scores for
        # incorrect or minimal movements
        # ========================================================================
        # Previous issue: 51% threshold was too lenient
        # - Even when stationary, model could predict 51-60% ‚Üí judgment=2 (66.7Ï†ê)
        # - Resulted in unfair scores: moving=75Ï†ê vs stationary=73Ï†ê
        #
        # New stricter criteria:
        # - 90%+ ‚Üí 3Ï†ê (Perfect, 100Ï†ê)
        # - 75%+ ‚Üí 2Ï†ê (Good, 66.7Ï†ê)
        # - 60%+ ‚Üí 1Ï†ê (Needs improvement, 33.3Ï†ê)
        # - <60% ‚Üí 0Ï†ê (Incorrect or no movement, 0Ï†ê)
        # ========================================================================
        if probability >= 0.90:
            return 3
        if probability >= 0.75:
            return 2
        if probability >= 0.60:
            return 1
        return 0

    def _fallback_score(
        self, predicted_label: str, confidence: float, target_action: str | None
    ) -> int:
        # ========================================================================
        # Fallback scoring when target_probability is not available
        # Aligned with stricter _score_by_probability criteria
        # ========================================================================
        if not target_action:
            # No target specified - use general confidence thresholds
            if confidence >= 0.90:
                return 3
            if confidence >= 0.75:
                return 2
            if confidence >= 0.60:
                return 1
            return 0

        target_key = target_action.strip().upper()
        predicted_key = predicted_label.strip().upper()

        if target_key == predicted_key:
            # Predicted correctly - use confidence thresholds
            if confidence >= 0.90:
                return 3
            if confidence >= 0.75:
                return 2
            if confidence >= 0.60:
                return 1
            return 0
        else:
            # Predicted incorrectly - always 0 points
            return 0

    # ========================================================================
    # ‚ö†Ô∏è CRITICAL: Filter out invalid frames (zero vectors) to prevent
    # meaningless predictions when person is not detected by Mediapipe
    # ========================================================================
    # Issue: When user doesn't move or is out of frame, Mediapipe returns
    # zero vectors, but model still predicts with low confidence (~15-20%)
    # This causes unfair scoring where "no movement" gets ~33-50 points!
    #
    # Solution: Only use frames where person is actually detected
    # Require minimum 3 valid frames to ensure reliable prediction
    # ========================================================================
    def _frames_to_keypoints(self, frames: Iterable[str]) -> Tuple[np.ndarray, float, float]:
        keypoints = []
        valid_count = 0
        total_count = 0

        decode_elapsed = 0.0
        pose_elapsed = 0.0
        for encoded in frames:
            total_count += 1
            decode_start = perf_counter()
            image = self._decode_base64_image(encoded)
            decode_elapsed += perf_counter() - decode_start

            pose_start = perf_counter()
            coords = self.pose_extractor.extract(image)
            pose_elapsed += perf_counter() - pose_start

            # Skip zero vectors (person not detected)
            if np.any(coords):
                keypoints.append(coords)
                valid_count += 1

        # Require at least 5 valid frames for reliable prediction
        # (Increased from 3 to reduce false positives when stationary)
        MIN_VALID_FRAMES = 5
        if valid_count < MIN_VALID_FRAMES:
            raise ValueError(
                f"Ïú†Ìö®Ìïú ÎèôÏûë ÌîÑÎ†àÏûÑÏù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§ ({valid_count}/{total_count}Í∞ú). "
                f"Ïπ¥Î©îÎùºÏóê Ï†ÑÏã†Ïù¥ Î≥¥Ïù¥ÎèÑÎ°ù Ìï¥Ï£ºÏÑ∏Ïöî."
            )

        LOGGER.info(
            "üìπ ÌîÑÎ†àÏûÑ Î∂ÑÏÑù: Ïú†Ìö®=%dÍ∞ú, Ï†ÑÏ≤¥=%dÍ∞ú, ÌïÑÌÑ∞ÎßÅ=%dÍ∞ú (ÏòÅÎ≤°ÌÑ∞ Ï†úÏô∏)",
            valid_count, total_count, total_count - valid_count
        )

        keypoint_array = np.stack(keypoints, axis=0).astype(np.float32)
        return keypoint_array, decode_elapsed, pose_elapsed

    def _sample_frames(self, frames: Sequence[str], target_count: int) -> List[str]:
        if len(frames) == target_count:
            return list(frames)

        if len(frames) < target_count:
            padding = [frames[-1]] * (target_count - len(frames))
            return list(frames) + padding

        indices = np.linspace(0, len(frames) - 1, target_count).astype(int)
        return [frames[i] for i in indices]

    @staticmethod
    def _decode_base64_image(data: str) -> np.ndarray:
        try:
            image_data = base64.b64decode(data)
        except base64.binascii.Error as exc:
            raise ValueError("Base64 ÎîîÏΩîÎî©Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.") from exc

        with Image.open(BytesIO(image_data)) as img:
            rgb_image = img.convert("RGB")
            return np.array(rgb_image)


@lru_cache(maxsize=1)
def get_inference_service() -> MotionInferenceService:
    model_path = Path(__file__).resolve().parent.parent / "trained_model" / "gcn_cnn_best.pt"
    device_override = os.getenv("MOTION_INFERENCE_DEVICE")
    return MotionInferenceService(model_path=model_path, device=device_override)


